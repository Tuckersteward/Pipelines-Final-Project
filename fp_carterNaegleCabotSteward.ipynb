{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "ez8swqb6HTx5",
    "outputId": "508ce238-c03f-4f6e-db32-4a11c54a285e"
   },
   "outputs": [],
   "source": [
    "# Carter Naegle & Cabot Steward\n",
    "\n",
    "# READ ME\n",
    "\n",
    "# We decided to put all the connection info into a .env inorder to put this on github more easily\n",
    "# we used the dotenv library\n",
    " \n",
    "# I added logging instead of the print statements we've used throughout class as they are much easier\n",
    "# to navigate if you have an issue or want to CTRL + F to find information, we did this with the loggin\n",
    "# package.  Plus they are faster\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import psycopg2\n",
    "import pymssql\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "environment = 'prod'\n",
    "# 'test' or 'prod'\n",
    "\n",
    "load_dotenv()\n",
    "log_folder = 'logging'\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "def get_connections():\n",
    "    try:\n",
    "        post_conn = psycopg2.connect(\n",
    "            database=os.getenv('POSTGRES_DB'),\n",
    "            user=os.getenv('POSTGRES_USER'),\n",
    "            password=os.getenv('POSTGRES_PASSWORD'),\n",
    "            host=os.getenv('POSTGRES_HOST'),\n",
    "            port=os.getenv('POSTGRES_PORT')\n",
    "        )\n",
    "        post_cursor = post_conn.cursor()\n",
    "        if environment == 'prod':\n",
    "            schema = 'prod'\n",
    "        else:\n",
    "            schema = 'test'\n",
    "\n",
    "        post_cursor.execute(f\"SET search_path TO {schema};\")\n",
    "\n",
    "\n",
    "        ssms_conn = pymssql.connect(\n",
    "            server=os.getenv('MSSQL_SERVER'),\n",
    "            user=os.getenv('MSSQL_USER'),\n",
    "            password=os.getenv('MSSQL_PASSWORD'),\n",
    "            database=os.getenv('MSSQL_DB')\n",
    "        )\n",
    "        ssms_cursor = ssms_conn.cursor()\n",
    "\n",
    "        return {\n",
    "            'post_conn': post_conn,\n",
    "            'post_cursor': post_cursor,\n",
    "            'ssms_conn': ssms_conn,\n",
    "            'ssms_cursor': ssms_cursor\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Att1rARI_yLJ",
    "outputId": "8253bdd4-daf8-4cd3-9add-5974619ec06b"
   },
   "outputs": [],
   "source": [
    "connections = get_connections()\n",
    "\n",
    "if connections:\n",
    "    post_conn = connections['post_conn']\n",
    "    post_cursor = connections['post_cursor']\n",
    "    ssms_conn = connections['ssms_conn']\n",
    "    ssms_cursor = connections['ssms_cursor']\n",
    "\n",
    "tables = [\n",
    "  \"\"\"CREATE TABLE IF NOT EXISTS weather_station (\n",
    "    stadium_weather_station_id SERIAL PRIMARY KEY,\n",
    "    stadium_weather_station_code VARCHAR(20),\n",
    "    station_name VARCHAR(70),\n",
    "    latitude NUMERIC(9,5),\n",
    "    longitude NUMERIC(9,5),\n",
    "    elevation NUMERIC(7,2)\n",
    "  );\"\"\",\n",
    "  \"\"\"CREATE TABLE IF NOT EXISTS nfl_stadium (\n",
    "    stadium_id SERIAL PRIMARY KEY,\n",
    "    stadium_name VARCHAR(50) NOT NULL,\n",
    "    stadium_city VARCHAR(50),\n",
    "    stadium_state VARCHAR(20),\n",
    "    stadium_open_year DATE,\n",
    "    stadium_close_year DATE,\n",
    "    stadium_type VARCHAR(15),\n",
    "    stadium_surface VARCHAR(50),\n",
    "    stadium_capacity INT,\n",
    "    stadium_weather_type VARCHAR(10),\n",
    "    stadium_weather_station_id INT,\n",
    "    CONSTRAINT FK_nfl_stadium_weather_station_id\n",
    "      FOREIGN KEY (stadium_weather_station_id)\n",
    "      REFERENCES weather_station(stadium_weather_station_id)\n",
    "  );\"\"\",\n",
    "  \"\"\"CREATE TABLE IF NOT EXISTS customer (\n",
    "    customer_id SERIAL PRIMARY KEY,\n",
    "    customer_first_name VARCHAR(50) NOT NULL,\n",
    "    customer_last_name VARCHAR(50) NOT NULL,\n",
    "    customer_age SMALLINT NOT NULL,\n",
    "    customer_type VARCHAR(25) NOT NULL,\n",
    "    customer_since DATE NOT NULL,\n",
    "    customer_income INT,\n",
    "    household_size SMALLINT,\n",
    "    mode_color VARCHAR(10),\n",
    "    source_customer_id INT\n",
    "  );\"\"\",\n",
    "  \"\"\"CREATE TABLE IF NOT EXISTS teams (\n",
    "    team_id SERIAL PRIMARY KEY,\n",
    "    team_name VARCHAR(50) NOT NULL,\n",
    "    team_name_short VARCHAR(25) NOT NULL,\n",
    "    team_abv VARCHAR(4) NOT NULL,\n",
    "    team_abv_pfr VARCHAR(4) NOT NULL,\n",
    "    team_conference VARCHAR(4) NOT NULL,\n",
    "    team_division VARCHAR(12),\n",
    "    team_conference_pre2002 VARCHAR(4) NOT NULL,\n",
    "    team_division_pre2002 VARCHAR(15)\n",
    "  );\"\"\",\n",
    "    \"\"\"CREATE TABLE IF NOT EXISTS game (\n",
    "      game_outcome_id SERIAL PRIMARY KEY,\n",
    "      schedule_date DATE NOT NULL,\n",
    "      schedule_season INT NOT NULL,\n",
    "      schedule_week VARCHAR(12) NOT NULL,\n",
    "      schedule_playoff BOOLEAN NOT NULL,\n",
    "      team_id_home INT NOT NULL,\n",
    "      score_home SMALLINT NOT NULL,\n",
    "      team_id_away INT NOT NULL,\n",
    "      score_away SMALLINT NOT NULL,\n",
    "      winner_ou VARCHAR(6) NOT NULL,\n",
    "      winner_line VARCHAR(8) NOT NULL,\n",
    "      team_favored INT,\n",
    "      favored_spread NUMERIC(3,1) NOT NULL,\n",
    "      over_under_line NUMERIC(3,1) NOT NULL,\n",
    "      stadium_id INT NOT NULL,\n",
    "      stadium_neutral BOOLEAN NOT NULL,\n",
    "      weather_temperature SMALLINT,\n",
    "      weather_wind_mph SMALLINT,\n",
    "      weather_humidity SMALLINT,\n",
    "      weather_detail VARCHAR(40),\n",
    "      CONSTRAINT FK_game_stadium_id\n",
    "        FOREIGN KEY (stadium_id)\n",
    "        REFERENCES nfl_stadium(stadium_id),\n",
    "      CONSTRAINT FK_game_team_id_home\n",
    "        FOREIGN KEY (team_id_home)\n",
    "        REFERENCES teams(team_id),\n",
    "      CONSTRAINT FK_game_team_id_away\n",
    "        FOREIGN KEY (team_id_away)\n",
    "        REFERENCES teams(team_id),\n",
    "        CONSTRAINT FK_game_team_favored\n",
    "        FOREIGN KEY (team_favored)\n",
    "        REFERENCES teams(team_id)\n",
    "    );\"\"\",\n",
    "  \"\"\"CREATE TABLE IF NOT EXISTS placed_bet (\n",
    "    bet_id SERIAL PRIMARY KEY,\n",
    "    customer_id INT NOT NULL,\n",
    "    game_outcome_id INT NOT NULL,\n",
    "    bet_amount SMALLINT NOT NULL,\n",
    "    bet_result VARCHAR(4),\n",
    "    commision_paid NUMERIC(8,2) NOT NULL,\n",
    "    bet_on VARCHAR(40),\n",
    "    bet_type VARCHAR(10),\n",
    "    CONSTRAINT FK_placed_bet_customer_id\n",
    "      FOREIGN KEY (customer_id)\n",
    "      REFERENCES customer(customer_id),\n",
    "    CONSTRAINT FK_placed_bet_game_outcome_id\n",
    "      FOREIGN KEY (game_outcome_id)\n",
    "      REFERENCES game(game_outcome_id)\n",
    "  );\"\"\"\n",
    "]\n",
    "\n",
    "for i in tables:\n",
    "  post_cursor.execute(i)\n",
    "\n",
    "post_conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tucke\\AppData\\Local\\Temp\\ipykernel_29712\\241823558.py:21: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, ssms_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Data Ingested Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Populate Customer Table\n",
    "connections = get_connections()\n",
    "\n",
    "if connections:\n",
    "    post_conn = connections['post_conn']\n",
    "    post_cursor = connections['post_cursor']\n",
    "    ssms_conn = connections['ssms_conn']\n",
    "    ssms_cursor = connections['ssms_cursor']\n",
    "\n",
    "log_file = os.path.join(log_folder, f\"customer_upload_{datetime.now().strftime('%Y%m%d')}.log\")\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filemode='w'  \n",
    ")\n",
    "    \n",
    "query = \"\"\"SELECT * FROM customer_table \"\"\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_sql(query, ssms_conn)\n",
    "    df[['customer_first_name', 'customer_last_name']] = df['customer_name'].str.split(n=1, expand=True)\n",
    "    df['customer_since'] = df['customer_since'].apply(lambda x: datetime(int(x), 1, 1).strftime('%Y-%m-%d'))\n",
    "\n",
    "    df.drop('customer_name', axis=1, inplace=True)\n",
    "    df.rename(columns={'customer_id': 'source_customer_id'}, inplace=True)\n",
    "\n",
    "    db_column_order = [\n",
    "        'customer_first_name',\n",
    "        'customer_last_name',\n",
    "        'customer_age',\n",
    "        'customer_type',\n",
    "        'customer_since',\n",
    "        'customer_income',\n",
    "        'household_size',\n",
    "        'mode_color',\n",
    "        'source_customer_id'\n",
    "    ]\n",
    "    \n",
    "    df = df[db_column_order]\n",
    "\n",
    "    temp_csv = 'customer_temp.csv'\n",
    "    df.to_csv(temp_csv, index=False)\n",
    "\n",
    "    with open(temp_csv, 'r') as file:\n",
    "        post_cursor.copy_expert(\n",
    "            \"\"\"\n",
    "            COPY customer (\n",
    "                customer_first_name,\n",
    "                customer_last_name,\n",
    "                customer_age,\n",
    "                customer_type,\n",
    "                customer_since,\n",
    "                customer_income,\n",
    "                household_size,\n",
    "                mode_color,\n",
    "                source_customer_id\n",
    "            )\n",
    "            FROM STDIN WITH CSV HEADER\n",
    "            \"\"\",\n",
    "            file\n",
    "        )\n",
    "    post_conn.commit()\n",
    "    logging.info(\"All customer data has been successfully inserted.\")\n",
    "    print(\"All Data Ingested Successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred during customer data ingestion: {e}\")\n",
    "    post_conn.rollback()\n",
    "\n",
    "finally:\n",
    "    if 'temp_csv' in locals() and os.path.exists(temp_csv):\n",
    "        os.remove(temp_csv)\n",
    "        logging.info(f\"Temporary file '{temp_csv}' has been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting Teams\n",
    "connections = get_connections()\n",
    "\n",
    "if connections:\n",
    "    post_conn = connections['post_conn']\n",
    "    post_cursor = connections['post_cursor']\n",
    "    ssms_conn = connections['ssms_conn']\n",
    "    ssms_cursor = connections['ssms_cursor']\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "log_file = os.path.join(log_folder, f\"teams_{datetime.now().strftime('%Y%m%d')}.log\")\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filemode='w' \n",
    ")\n",
    "\n",
    "csv_teams = 'nfl_teams.csv'\n",
    "teams_df = pd.read_csv(csv_teams)\n",
    "\n",
    "teams_df['team_id'] = teams_df['team_id'].replace({'LVR': 'LV'})\n",
    "\n",
    "temp_csv = 'teams_temp.csv'\n",
    "teams_df.to_csv(temp_csv, index=False)\n",
    "\n",
    "try:\n",
    "    with open(temp_csv, 'r') as file:\n",
    "        post_cursor.copy_expert(\n",
    "            \"\"\"\n",
    "            COPY teams (\n",
    "                team_name,\n",
    "                team_name_short,\n",
    "                team_abv,\n",
    "                team_abv_pfr,\n",
    "                team_conference,\n",
    "                team_division,\n",
    "                team_conference_pre2002,\n",
    "                team_division_pre2002\n",
    "            )\n",
    "            FROM STDIN WITH CSV HEADER\n",
    "            \"\"\",\n",
    "            file\n",
    "        )\n",
    "    post_conn.commit()\n",
    "    logging.info(\"All team data has been successfully inserted.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error occurred during insertion: {e}\")\n",
    "    post_conn.rollback()\n",
    "finally:\n",
    "    if os.path.exists(temp_csv):\n",
    "        os.remove(temp_csv)\n",
    "        logging.info(f\"Temporary file '{temp_csv}' has been deleted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather stations\n",
    "connections = get_connections()\n",
    "\n",
    "if connections:\n",
    "    post_conn = connections['post_conn']\n",
    "    post_cursor = connections['post_cursor']\n",
    "    ssms_conn = connections['ssms_conn']\n",
    "    ssms_cursor = connections['ssms_cursor']\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "log_file = os.path.join(log_folder, f\"weather_stations_{datetime.now().strftime('%Y%m%d')}.log\")\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filemode='w'  \n",
    ")\n",
    "\n",
    "csv_stadiums = 'nfl_stadiums.csv'\n",
    "temp_csv_file = 'temp_weather_station_upload.csv'\n",
    "\n",
    "weather_df = pd.read_csv(csv_stadiums, encoding='latin1')\n",
    "weather_df = weather_df.dropna(subset=['STATION']).drop_duplicates(subset=['STATION'])\n",
    "\n",
    "bulk_data = []\n",
    "error_rows = []\n",
    "\n",
    "for index, row in weather_df.iterrows():\n",
    "    try:\n",
    "        post_cursor.execute(\n",
    "            'SELECT count(*) FROM weather_station WHERE stadium_weather_station_code = %s;', \n",
    "            (row['STATION'],)\n",
    "        )\n",
    "        result = post_cursor.fetchone()\n",
    "\n",
    "        if result[0] != 0:\n",
    "            logging.error(f\"Weather station {row['STATION']} already exists, skipping.\")\n",
    "            continue\n",
    "\n",
    "        bulk_data.append([\n",
    "            row['STATION'],  \n",
    "            row['NAME'],    \n",
    "            float(row['LATITUDE']),  \n",
    "            float(row['LONGITUDE']),  \n",
    "            float(row['ELEVATION'])  \n",
    "        ])\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing row {index}: {e} {row.to_dict()}\")\n",
    "        error_rows.append(row.to_dict())\n",
    "\n",
    "with open(temp_csv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['stadium_weather_station_code', 'station_name', 'latitude', 'longitude', 'elevation'])\n",
    "    writer.writerows(bulk_data)\n",
    "\n",
    "try:\n",
    "    with open(temp_csv_file, 'r') as file:\n",
    "        post_cursor.copy_expert(\n",
    "            \"\"\"COPY weather_station (\n",
    "                stadium_weather_station_code, station_name, latitude, longitude, elevation\n",
    "            ) FROM STDIN WITH CSV HEADER\"\"\",\n",
    "            file\n",
    "        )\n",
    "    post_conn.commit()\n",
    "    logging.info(f\"Successfully inserted bulk data from {temp_csv_file}\")\n",
    "except Exception as e:\n",
    "    post_conn.rollback()\n",
    "    logging.error(f\"Error during bulk insert: {e}\")\n",
    "finally:\n",
    "    if os.path.exists(temp_csv_file):\n",
    "        os.remove(temp_csv_file)\n",
    "        logging.info(f\"Temporary file '{temp_csv_file}' deleted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stadium\n",
    "connections = get_connections()\n",
    "\n",
    "if connections:\n",
    "    post_conn = connections['post_conn']\n",
    "    post_cursor = connections['post_cursor']\n",
    "    ssms_conn = connections['ssms_conn']\n",
    "    ssms_cursor = connections['ssms_cursor']\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "log_file = os.path.join(log_folder, f\"stadium_{datetime.now().strftime('%Y%m%d')}.log\")\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filemode='w' \n",
    ")\n",
    "\n",
    "csv_stadiums = 'nfl_stadiums.csv'\n",
    "bulk_data = []\n",
    "error_rows = []\n",
    "\n",
    "stadium_df = pd.read_csv(csv_stadiums, encoding='latin1')\n",
    "stadium_df[['city', 'state']] = stadium_df['stadium_location'].str.split(', ', expand=True)\n",
    "\n",
    "\n",
    "for index, row in stadium_df.iterrows():   \n",
    "    try:\n",
    "        post_cursor.execute(\n",
    "                'select count(*) from nfl_stadium where stadium_name = %s;', \n",
    "                (row['stadium_name'],)\n",
    "            )\n",
    "        result = post_cursor.fetchone()\n",
    "\n",
    "        if result[0] != 0:\n",
    "            logging.error(f\"This record already exists: {row.to_dict()}\")\n",
    "            continue\n",
    "        \n",
    "        stadium_open = None\n",
    "        if pd.notna(row['stadium_open']):\n",
    "            stadium_open = datetime.date(int(row['stadium_open']), 1, 1)\n",
    "\n",
    "        stadium_close = None\n",
    "        if pd.notna(row['stadium_close']):\n",
    "            stadium_close = datetime.date(int(row['stadium_close']), 1, 1)\n",
    "\n",
    "        stadium_capacity = None\n",
    "        if pd.notna(row['stadium_capacity']):\n",
    "            stadium_capacity = int(row['stadium_capacity'].replace(',', ''))\n",
    "\n",
    "\n",
    "        stadium_weather_station_id = None\n",
    "        if pd.notna(row['STATION']):\n",
    "            post_cursor.execute(\n",
    "                'SELECT stadium_weather_station_id FROM weather_station WHERE stadium_weather_station_code = %s;', \n",
    "                (row['STATION'],)\n",
    "            )\n",
    "            weather_result = post_cursor.fetchone()\n",
    "            if weather_result:\n",
    "                stadium_weather_station_id = weather_result[0]\n",
    "\n",
    "        stadium_surface = None\n",
    "        if pd.notna(row['stadium_surface']):\n",
    "            stadium_surface = row['stadium_surface'].split(',')[0].strip()\n",
    "\n",
    "        stadium_weather_type = None\n",
    "        if pd.notna(row['stadium_weather_type']):\n",
    "            stadium_weather_type = row['stadium_capacity']\n",
    "\n",
    "        stadium_type = None\n",
    "        if pd.notna(row['stadium_type']):\n",
    "            stadium_type = row['stadium_type']\n",
    "\n",
    "        bulk_data.append([\n",
    "                stadium_weather_station_id,\n",
    "                row['stadium_name'],\n",
    "                row['city'],\n",
    "                row['state'],\n",
    "                stadium_open,\n",
    "                stadium_close,\n",
    "                stadium_type,\n",
    "                stadium_surface,\n",
    "                stadium_capacity,\n",
    "                stadium_weather_type\n",
    "            ])\n",
    "        logging.info(f\"Stadium {row['stadium_name']} is ready to be written to csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing row {index}: {e} {row.to_dict()}\")\n",
    "        error_rows.append(row.to_dict())\n",
    "\n",
    "temp_csv_file = 'temp_nfl_stadium_upload.csv'\n",
    "\n",
    "with open(temp_csv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\n",
    "        'stadium_weather_station_id', 'stadium_name', 'stadium_city', \n",
    "        'stadium_state', 'stadium_open_year', 'stadium_close_year', \n",
    "        'stadium_type', 'stadium_surface', 'stadium_capacity', 'stadium_weather_type'\n",
    "    ])\n",
    "    writer.writerows(bulk_data)\n",
    "    logging.info(f\"Successfully wrote all data to csv: {temp_csv_file}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(temp_csv_file, 'r') as file:\n",
    "        post_cursor.copy_expert(\n",
    "            \"\"\"COPY nfl_stadium (\n",
    "                stadium_weather_station_id, stadium_name, stadium_city, \n",
    "                stadium_state, stadium_open_year, stadium_close_year, \n",
    "                stadium_type, stadium_surface, stadium_capacity, stadium_weather_type\n",
    "            ) FROM STDIN WITH CSV HEADER\"\"\",\n",
    "            file\n",
    "        )\n",
    "    post_conn.commit()\n",
    "    logging.info(f\"Successfully inserted bulk data from {temp_csv_file}\")\n",
    "    os.remove(temp_csv_file)\n",
    "except Exception as e:\n",
    "    post_conn.rollback()\n",
    "    logging.error(f\"Error during bulk insert: {e}\")\n",
    "finally:\n",
    "    pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows imported with no failures.\n"
     ]
    }
   ],
   "source": [
    "# game\n",
    "\n",
    "connections = get_connections()\n",
    "\n",
    "if connections:\n",
    "    post_conn = connections['post_conn']\n",
    "    post_cursor = connections['post_cursor']\n",
    "    ssms_conn = connections['ssms_conn']\n",
    "    ssms_cursor = connections['ssms_cursor']\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "log_file = os.path.join(log_folder, f\"game_{datetime.now().strftime('%Y%m%d')}.log\")\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filemode='w' \n",
    ")\n",
    "\n",
    "csv_file = 'spread_scores-2.csv'\n",
    "game = pd.read_csv(csv_file)\n",
    "game = game[game['schedule_season'] >= 2015]\n",
    "\n",
    "error_csv = 'failed_inserts.csv'\n",
    "error_rows = []\n",
    "\n",
    "nullable = ['weather_temperature', 'weather_wind_mph', 'weather_humidity', 'weather_detail', 'team_favorite_id']\n",
    "game[nullable] = game[nullable].replace({np.nan: None})\n",
    "\n",
    "game['schedule_date'] = pd.to_datetime(game['schedule_date'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "def get_team_id(values):\n",
    "    post_cursor.execute(\n",
    "        'SELECT team_id FROM teams WHERE team_name = %s;',\n",
    "        (values,)\n",
    "    )\n",
    "    team_id = post_cursor.fetchone()\n",
    "    return team_id[0] if team_id else None\n",
    "\n",
    "def get_team_favorite_id(values):\n",
    "    if values == 'PICK':\n",
    "        return None\n",
    "    post_cursor.execute(\n",
    "        'SELECT team_id FROM teams WHERE team_abv = %s;',\n",
    "        (values,)\n",
    "    )\n",
    "    team_favorite_id = post_cursor.fetchone()\n",
    "    return team_favorite_id[0] if team_favorite_id else None\n",
    "\n",
    "def get_stadium_id(values):\n",
    "    post_cursor.execute(\n",
    "        'SELECT stadium_id FROM nfl_stadium WHERE stadium_name = %s;',\n",
    "        (values,)\n",
    "    )\n",
    "    stadium_id = post_cursor.fetchone()\n",
    "\n",
    "    if stadium_id == None:\n",
    "        insert_stadium = (\n",
    "            \"\"\"insert into nfl_stadium (stadium_name) values (%s) returning stadium_id\"\"\"\n",
    "        )\n",
    "        post_cursor.execute(insert_stadium, (values,))\n",
    "        stadium_id = post_cursor.fetchone()\n",
    "\n",
    "    return stadium_id[0]\n",
    "\n",
    "def calculate_betting_results(row):\n",
    "    total_points = row['score_home'] + row['score_away']\n",
    "    if total_points > row['over_under_line']:\n",
    "        winner_ou = 'over'\n",
    "    elif total_points < row['over_under_line']:\n",
    "        winner_ou = 'under'\n",
    "    else:\n",
    "        winner_ou = 'push'\n",
    "\n",
    "    # Spread\n",
    "    margin = (\n",
    "        row['score_home'] - row['score_away']\n",
    "        if row['team_favorite_id'] == row['team_home']\n",
    "        else row['score_away'] - row['score_home']\n",
    "    )\n",
    "    if margin > row['spread_favorite']:\n",
    "        winner_line = 'favored'\n",
    "    elif margin < row['spread_favorite']:\n",
    "        winner_line = 'underdog'\n",
    "    else:\n",
    "        winner_line = 'push'\n",
    "\n",
    "    return winner_ou, winner_line\n",
    "\n",
    "def parse_int(values, allow_negative=False):\n",
    "# allow_negative: True, allow negative integers; if False, return None for negatives.\n",
    "    try:\n",
    "        result = int(values)\n",
    "        if not allow_negative and result < 0:\n",
    "            return None\n",
    "        return result\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "for index, row in game.iterrows():\n",
    "    query = \"\"\"select count(*) \n",
    "                from game \n",
    "                where team_id_home in (\n",
    "                \t\tselect team_id\n",
    "                \t\tfrom teams \n",
    "                \t\twhere team_name = %s)\n",
    "                \tand schedule_date = %s\"\"\"\n",
    "    post_cursor.execute(query, (row['team_home'], row['schedule_date']))\n",
    "    duplicate_game = post_cursor.fetchone()\n",
    "\n",
    "    if duplicate_game[0] > 0:\n",
    "        logging.warning(f\"game already exists for {row['team_home']} {row['schedule_date']}.\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        team_id_home = get_team_id(row['team_home'])\n",
    "        team_id_away = get_team_id(row['team_away'])\n",
    "        stadium_id = get_stadium_id(row['stadium'])\n",
    "        team_id_favorite = get_team_favorite_id(row['team_favorite_id'])\n",
    "        weather_temperature = parse_int(row['weather_temperature'], allow_negative=True)\n",
    "        weather_wind = parse_int(row['weather_wind_mph'])\n",
    "        weather_humidity = parse_int(row['weather_humidity'])\n",
    "\n",
    "        weather_detail = None\n",
    "        if pd.notna(row['weather_detail']):\n",
    "            weather_detail = row['weather_detail']\n",
    "\n",
    "        winner_ou, winner_line = calculate_betting_results(row)\n",
    "\n",
    "\n",
    "        game_insert_query = \"\"\"INSERT INTO game (schedule_date,\n",
    "                                                schedule_season,\n",
    "                                                schedule_week,\n",
    "                                                schedule_playoff,\n",
    "                                                team_id_home,\n",
    "                                                score_home,\n",
    "                                                team_id_away,\n",
    "                                                score_away,\n",
    "                                                winner_ou,\n",
    "                                                winner_line,\n",
    "                                                team_favored,\n",
    "                                                favored_spread,\n",
    "                                                over_under_line,\n",
    "                                                stadium_id,\n",
    "                                                stadium_neutral,\n",
    "                                                weather_temperature,\n",
    "                                                weather_wind_mph,\n",
    "                                                weather_humidity,\n",
    "                                                weather_detail)\n",
    "                                  VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\"\n",
    "\n",
    "        post_cursor.execute(game_insert_query, (\n",
    "            row['schedule_date'],\n",
    "            row['schedule_season'],\n",
    "            row['schedule_week'],\n",
    "            row['schedule_playoff'],\n",
    "            team_id_home,\n",
    "            row['score_home'],\n",
    "            team_id_away,\n",
    "            row['score_away'],\n",
    "            winner_ou,\n",
    "            winner_line,\n",
    "            team_id_favorite,\n",
    "            row['spread_favorite'],\n",
    "            row['over_under_line'],\n",
    "            stadium_id,\n",
    "            row['stadium_neutral'],\n",
    "            weather_temperature,\n",
    "            weather_wind,\n",
    "            weather_humidity,\n",
    "            weather_detail\n",
    "        ))\n",
    "\n",
    "        post_conn.commit()\n",
    "\n",
    "        logging.info(f\"Successfully uploaded record: {row.to_dict()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing row {index}: {e} {row.to_dict()}\")\n",
    "        error_rows.append(row.to_dict())\n",
    "        post_conn.rollback()\n",
    "\n",
    "post_conn.commit()\n",
    "\n",
    "\n",
    "if error_rows:\n",
    "    with open(error_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = game.columns\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(error_rows)\n",
    "\n",
    "logging.info(\"Game upload complete.\")\n",
    "\n",
    "if error_rows:\n",
    "    logging.warning(f\"Some rows failed. Check {error_csv} for details.\")\n",
    "    print('Some rows failed, check logging')\n",
    "else:\n",
    "    print('All rows imported with no failures.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tucke\\AppData\\Local\\Temp\\ipykernel_29712\\3240084397.py:67: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, ssms_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion Successfull\n",
      "successfully moved out of staging\n"
     ]
    }
   ],
   "source": [
    "# placed bet\n",
    "connections = get_connections()\n",
    "\n",
    "if connections:\n",
    "    post_conn = connections['post_conn']\n",
    "    post_cursor = connections['post_cursor']\n",
    "    ssms_conn = connections['ssms_conn']\n",
    "    ssms_cursor = connections['ssms_cursor']\n",
    "    \n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "log_file = os.path.join(log_folder, f\"placed_bet_{datetime.now().strftime('%Y%m%d')}.log\")\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filemode='w' \n",
    ")\n",
    "# removed TEMP from it for now\n",
    "temp_table = \"\"\"CREATE TEMP TABLE IF NOT EXISTS placed_bet_staging (\n",
    "    bet_amount SMALLINT NOT NULL,\n",
    "    commission_paid NUMERIC(8,2),\n",
    "    bet_on VARCHAR(40),\n",
    "    bet_type VARCHAR(10),\n",
    "    customer_name VARCHAR(50),\n",
    "    game_id VARCHAR(30),\n",
    "    source_customer_id INT,\n",
    "    ingested BOOLEAN DEFAULT FALSE\n",
    ");\"\"\"\n",
    "\n",
    "post_cursor.execute(temp_table)\n",
    "\n",
    "post_conn.commit()\n",
    "\n",
    "query = \"\"\"\n",
    "with cte as (\n",
    "SELECT \n",
    "    bet_amount,  \n",
    "    CASE\n",
    "        WHEN bet_amount <= 1000 THEN bet_amount * 0.1\n",
    "        WHEN bet_amount <= 4000 THEN (1000 * 0.1) + ((bet_amount - 1000) * 0.08)\n",
    "        ELSE (1000 * 0.1) + (3000 * 0.08) + ((bet_amount - 4000) * 0.06)\n",
    "    END AS commission_paid,\n",
    "    bet_on,\n",
    "    CASE \n",
    "        WHEN bet_on IN ('under', 'over', 'push') THEN LOWER(bet_on)\n",
    "        ELSE 'line' \n",
    "    END AS bet_type,\n",
    "    ct.customer_name,\n",
    "    game_id, \n",
    "    ct.customer_id\n",
    "FROM betlog bl\n",
    "INNER JOIN customer_table ct ON bl.customer_id = ct.customer_id\n",
    "where bet_amount is not NULL \n",
    "\tand bet_on is not NULL)\n",
    "select * \n",
    "from cte \n",
    "where commission_paid is not NULL\n",
    "\tand customer_name is not NULL \n",
    "\tand game_id is not NULL\n",
    "\tand customer_id is not NULL\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_sql(query, ssms_conn)\n",
    "    df = df.dropna(how='all')\n",
    "\n",
    "    csv_file = 'placed_bet_staging.csv'\n",
    "    df.to_csv(csv_file, index=False)\n",
    "\n",
    "    with open(csv_file, 'r') as file:\n",
    "        next(file) \n",
    "        post_cursor.copy_expert(\n",
    "            f\"\"\"\n",
    "            COPY placed_bet_staging (\n",
    "                bet_amount, commission_paid, bet_on, bet_type, customer_name, game_id, source_customer_id\n",
    "            )\n",
    "            FROM STDIN WITH CSV\n",
    "            DELIMITER ','\n",
    "            \"\"\",\n",
    "            file\n",
    "        )\n",
    "        post_conn.commit()\n",
    "\n",
    "    print(\"Ingestion Successfull\")\n",
    "    logging.info(\"Data successfully copied to PostgreSQL staging table.\")\n",
    "    os.remove(csv_file)\n",
    "    logging.info(f\"Temporary file '{csv_file}' deleted.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during data processing: {e}\")\n",
    "    logging.error(f\"Error during data processing: {e}\")\n",
    "\n",
    "finally:\n",
    "    pass\n",
    "\n",
    "stage_to_prod = \"\"\"\n",
    "WITH \n",
    "TeamOccurrences AS (\n",
    "    SELECT \n",
    "        t.team_id,\n",
    "        t.team_name,\n",
    "        t.team_abv,\n",
    "        COUNT(*) AS occurrences,\n",
    "        ROW_NUMBER() OVER (PARTITION BY t.team_name ORDER BY COUNT(*) DESC) AS row_num\n",
    "    FROM \n",
    "        teams t\n",
    "    INNER JOIN \n",
    "        game g \n",
    "        ON t.team_id = g.team_id_home\n",
    "    WHERE \n",
    "        g.schedule_date >= NOW() - INTERVAL '1 year'\n",
    "    GROUP BY \n",
    "        t.team_id, t.team_name),\n",
    "game_data AS (\n",
    "    SELECT \n",
    "        pbs.bet_amount,\n",
    "        pbs.commission_paid,\n",
    "        pbs.bet_on,\n",
    "        pbs.bet_type,\n",
    "        pbs.customer_name,\n",
    "        pbs.game_id,\n",
    "        pbs.source_customer_id,\n",
    "        th.team_id AS home_team_id,\n",
    "        th.team_abv AS home_team_abv,\n",
    "        ta.team_id AS away_team_id,\n",
    "        ta.team_abv AS away_team_abv,\n",
    "        g.game_outcome_id,\n",
    "        g.winner_line,\n",
    "        g.schedule_date,\n",
    "        g.score_home,\n",
    "        g.score_away,\n",
    "        c.customer_id,\n",
    "        g.favored_spread\n",
    "     FROM placed_bet_staging pbs\n",
    "    FULL OUTER JOIN TeamOccurrences th\n",
    "        ON th.team_abv = case when split_part(pbs.game_id, '-', 2) in ('JAC') then 'JAX' else split_part(pbs.game_id, '-', 2) end -- Home team abbreviation\n",
    "        and th.row_num = 1\n",
    "    FULL OUTER JOIN TeamOccurrences ta\n",
    "        ON ta.team_abv = case when split_part(pbs.game_id, '-', 3) in ('JAC') then 'JAX' else split_part(pbs.game_id, '-', 3) end  -- Away team abbreviation\n",
    "        and th.row_num = 1\n",
    "--        and team\n",
    "    full OUTER JOIN game g\n",
    "        ON g.schedule_season = LEFT(pbs.game_id, 4)::INT -- Extract year\n",
    "        AND CASE\n",
    "        \tWHEN schedule_week = 'Wildcard' THEN 19\n",
    "        \tWHEN schedule_week = 'Division' THEN 20\n",
    "        \tWHEN schedule_week = 'Conference' THEN 21\n",
    "        \tWHEN schedule_week = 'Superbowl' THEN 22\n",
    "        \tELSE schedule_week::INT end = right(split_part(pbs.game_id, '-', 1),2)::INT -- Extract week\n",
    "        AND g.team_id_home = th.team_id\n",
    "        AND g.team_id_away = ta.team_id\n",
    "    INNER JOIN customer c\n",
    "        ON c.customer_id = pbs.source_customer_id\n",
    "     where LEFT(split_part(pbs.game_id, '-', 1), 4)::INT > 2015\n",
    "),\n",
    "bet_outcomes AS (\n",
    "    SELECT \n",
    "        gd.customer_id,\n",
    "        gd.game_outcome_id,\n",
    "        gd.bet_amount,\n",
    "        CASE \n",
    "            WHEN gd.bet_type = 'line' AND gd.bet_on = gd.winner_line THEN 'win'\n",
    "            WHEN gd.bet_type = 'line' AND gd.bet_on != gd.winner_line THEN 'lose'\n",
    "            WHEN gd.bet_type = 'over' AND (gd.score_home + gd.score_away) > gd.favored_spread THEN 'win'\n",
    "            WHEN gd.bet_type = 'over' AND (gd.score_home + gd.score_away) <= gd.favored_spread THEN 'lose'\n",
    "            WHEN gd.bet_type = 'under' AND (gd.score_home + gd.score_away) < gd.favored_spread THEN 'win'\n",
    "            WHEN gd.bet_type = 'under' AND (gd.score_home + gd.score_away) >= gd.favored_spread THEN 'lose'\n",
    "            ELSE null\n",
    "        END AS bet_result,\n",
    "        gd.commission_paid,\n",
    "        gd.bet_on,\n",
    "        gd.bet_type\n",
    "    FROM game_data gd\n",
    ")\n",
    "INSERT INTO placed_bet (\n",
    "    customer_id,\n",
    "    game_outcome_id,\n",
    "    bet_amount,\n",
    "    bet_result,\n",
    "    commision_paid,\n",
    "    bet_on,\n",
    "    bet_type)\n",
    "    SELECT \n",
    "    customer_id,\n",
    "    game_outcome_id,\n",
    "    bet_amount,\n",
    "    bet_result,\n",
    "    commission_paid as commision_paid ,\n",
    "    bet_on,\n",
    "    bet_type\n",
    "FROM bet_outcomes;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    post_cursor.execute(stage_to_prod)\n",
    "    post_conn.commit()\n",
    "\n",
    "    print(\"successfully moved out of staging\")\n",
    "    logging.info('Inserting into prod successfully')\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")\n",
    "    logging.error(f\"Error during relpication processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
